import random

import numpy as np
import pandas as pd
import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader, Dataset


class DiscontinuityDataset(Dataset):
    def __init__(self, synthetic_data: pd.DataFrame, num_features: int = 10):
        """
        Args:
            synthetic_data: DataFrame containing the synthetic dataset,
                             generated by the generate_synthetic_data method.
        """
        self.dataframe = synthetic_data
        self.groups = self.dataframe.groupby("function")

        self.discontinuity_info = []

        for function_id, group in self.groups:
            x_values = group[[f"x{i+1}" for i in range(num_features)]].iloc[0].values
            discontinuity_times = group["discontinuity"].values
            self.discontinuity_info.append((function_id, x_values, discontinuity_times))

    def __len__(self):
        return len(self.groups)

    def __getitem__(self, idx):
        row = self.discontinuity_info[idx]

        x = torch.tensor(row[1], dtype=torch.float32)
        discontinuity_times = torch.tensor(row[2], dtype=torch.float32)

        return x, discontinuity_times


class DiscontinuityDataModule(pl.LightningDataModule):
    def __init__(
        self,
        dataframe: pd.DataFrame,
        batch_size: int = 32,
        num_features: int = 10,
        val_split: float = 0.2,
        test_split: float = 0.1,
    ):
        """
        Args:
            dataframe: The synthetic data as a Pandas DataFrame.
            batch_size: The batch size to use during training.
            max_discontinuities: The maximum number of discontinuities to predict.
            val_split: Proportion of data to use for validation.
        """
        super().__init__()
        self.dataframe = dataframe
        self.batch_size = batch_size
        self.val_split = val_split
        self.test_split = test_split
        self.num_features = num_features

        np.random.seed(42)
        random.seed(42)

    def setup(self, stage=None):
        functions = self.dataframe["function"].unique()
        indices = np.arange(len(functions))
        np.random.shuffle(indices)
        
        train_indices = functions[indices[: int(len(functions) * (1 - self.val_split - self.test_split))]]
        val_indices = functions[
            indices[
                int(len(functions) * (1 - self.val_split - self.test_split)) : int(
                    len(functions) * (1 - self.test_split)
                )
            ]
        ]
        test_indices = functions[indices[int(len(functions) * (1 - self.test_split)) :]]

        train_data = self.dataframe[self.dataframe["function"].isin(train_indices)]
        val_data = self.dataframe[self.dataframe["function"].isin(val_indices)]
        test_data = self.dataframe[self.dataframe["function"].isin(test_indices)]
        
        print(f"Train samples: {len(train_data)}")
        print(f"Val samples: {len(val_data)}")
        print(f"Test samples: {len(test_data)}")

        self.train_dataset = DiscontinuityDataset(train_data, self.num_features)
        self.val_dataset = DiscontinuityDataset(val_data, self.num_features)
        self.test_dataset = DiscontinuityDataset(test_data, self.num_features)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)
